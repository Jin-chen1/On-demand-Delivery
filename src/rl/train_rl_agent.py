"""
Day 13: PPOè®­ç»ƒæµæ°´çº¿
åŸºäºStable-Baselines3æ­å»ºå®Œæ•´çš„PPOè®­ç»ƒæµæ°´çº¿

åŠŸèƒ½ï¼š
1. TensorBoardç›‘æ§
2. è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼ˆè®­ç»ƒç›‘æ§ã€æ—©åœã€åŠ¨æ€è°ƒæ•´ï¼‰
3. è¯¾ç¨‹å­¦ä¹ æ”¯æŒï¼ˆä»ä½è´Ÿè½½åˆ°é«˜è´Ÿè½½ï¼‰
4. æ¨¡å‹ä¿å­˜ä¸æ¢å¤
5. åŸºçº¿å¯¹æ¯”è¯„ä¼°

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¯¾ç¨‹å­¦ä¹ å®ç°è¯´æ˜                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é¡¹ç›®ä¸­æœ‰ä¸¤å¥—è¯¾ç¨‹å­¦ä¹ å®ç°ï¼Œå„æœ‰é€‚ç”¨åœºæ™¯ï¼š                                â”‚
â”‚                                                                          â”‚
â”‚ ã€ç®€åŒ–ç‰ˆã€‘æœ¬æ¨¡å— - RLTrainer.train_with_curriculum()                    â”‚
â”‚   â”œâ”€â”€ ç‰¹ç‚¹ï¼šç›´æ¥è¯»å–YAMLé…ç½®ï¼Œæ”¯æŒè¾¾æ ‡è·³è½¬å’ŒåŠ æ—¶èµ›                      â”‚
â”‚   â”œâ”€â”€ é€‚ç”¨ï¼šå¿«é€Ÿå®éªŒã€åŸºç¡€è®­ç»ƒã€è®ºæ–‡å¤ç°                                â”‚
â”‚   â”œâ”€â”€ é…ç½®ï¼šrl_config.yaml â†’ training.curriculum.curriculum_stages     â”‚
â”‚   â””â”€â”€ çŠ¶æ€ï¼šâœ… å½“å‰ä¸»æµç¨‹ä½¿ç”¨                                           â”‚
â”‚                                                                          â”‚
â”‚ ã€å®Œæ•´ç‰ˆã€‘src/rl/curriculum_learning.py                                 â”‚
â”‚   â”œâ”€â”€ ç‰¹ç‚¹ï¼šå›é€€æœºåˆ¶ã€å¹³æ»‘è¿‡æ¸¡ã€å¤šç»´åº¦éš¾åº¦è¯„åˆ†                          â”‚
â”‚   â”œâ”€â”€ é€‚ç”¨ï¼šéœ€è¦ç²¾ç»†æ§åˆ¶è¯¾ç¨‹ç­–ç•¥çš„é«˜çº§åœºæ™¯                              â”‚
â”‚   â”œâ”€â”€ ç±»ï¼šCurriculumManager + CurriculumLearningCallback                â”‚
â”‚   â””â”€â”€ çŠ¶æ€ï¼šğŸ”¶ é¢„ç•™æ‰©å±•ï¼Œæœªæ¥å…¥ä¸»æµç¨‹                                   â”‚
â”‚                                                                          â”‚
â”‚ æ³¨æ„ï¼šä¸¤å¥—å®ç°ç›¸äº’ç‹¬ç«‹ï¼Œä¿®æ”¹ä¸€å¥—ä¸ä¼šå½±å“å¦ä¸€å¥—ï¼                        â”‚
â”‚ å¦‚éœ€ä½¿ç”¨å®Œæ•´ç‰ˆï¼Œè¯·å‚è€ƒcurriculum_learning.pyä¸­çš„ä½¿ç”¨è¯´æ˜ã€‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¾èµ–ï¼špip install stable-baselines3[extra] tensorboard
"""

import argparse
import yaml
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
import json
import time
from typing import Dict, Any, Optional, List, Callable

# RLè®­ç»ƒåº“ - ä½¿ç”¨ç»Ÿä¸€çš„SB3_AVAILABLEæ ‡å¿—
from . import SB3_AVAILABLE

if SB3_AVAILABLE:
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_checker import check_env
    from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
    from stable_baselines3.common.callbacks import (
        BaseCallback, 
        EvalCallback, 
        CheckpointCallback,
        CallbackList
    )
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.monitor import Monitor
else:
    print("è­¦å‘Šï¼šStable-Baselines3æœªå®‰è£…ï¼ŒRLè®­ç»ƒåŠŸèƒ½ä¸å¯ç”¨")
    print("å®‰è£…å‘½ä»¤ï¼špip install stable-baselines3[extra]")

from gymnasium import spaces
from .rl_environment import DeliveryRLEnvironment

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================
# æ¨¡å—çº§ç¯å¢ƒå·¥å‚å‡½æ•°ï¼ˆç”¨äºSubprocVecEnvçš„pickleå…¼å®¹ï¼‰
# ============================================================
# Windowså¤šè¿›ç¨‹å…¼å®¹æ€§è¯´æ˜ï¼š
# - Windowsä½¿ç”¨spawnæ¨¡å¼å¯åŠ¨å­è¿›ç¨‹ï¼Œè¦æ±‚ä¼ é€’ç»™SubprocVecEnvçš„å‡½æ•°å¿…é¡»å¯pickle
# - lambdaå‡½æ•°é€šå¸¸ä¸èƒ½è¢«pickleï¼Œå› æ­¤ç¯å¢ƒå·¥å‚å‡½æ•°å¿…é¡»åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰
# - å½“å‰å®ç°ï¼š
#   - _make_env_factoryåœ¨æ¨¡å—é¡¶å±‚ï¼Œå¯è¢«pickle âœ…
#   - train_with_curriculumä½¿ç”¨DummyVecEnvï¼ˆå•è¿›ç¨‹ï¼‰ï¼Œæ— pickleé—®é¢˜ âœ…
#   - CurriculumLearningCallback._update_environmentä½¿ç”¨DummyVecEnv âœ…
# - å¦‚æœæœªæ¥éœ€è¦ä½¿ç”¨SubprocVecEnvè¿›è¡ŒçœŸæ­£çš„å¤šè¿›ç¨‹è®­ç»ƒï¼Œéœ€è¦ç¡®ä¿ï¼š
#   - æ‰€æœ‰ç¯å¢ƒåˆ›å»ºå‡½æ•°éƒ½åœ¨æ¨¡å—é¡¶å±‚
#   - ä¸ä½¿ç”¨lambdaæˆ–é—­åŒ…æ•è·å¤æ‚å¯¹è±¡
# ============================================================

def _make_env_factory(sim_config: Dict[str, Any], rl_config: Dict[str, Any], 
                      log_dir: Optional[str] = None, rank: int = 0):
    """
    åˆ›å»ºç¯å¢ƒå·¥å‚å‡½æ•°ï¼ˆæ¨¡å—çº§ï¼Œå¯è¢«pickleï¼‰
    
    Windowså…¼å®¹æ€§ï¼š
    - æ­¤å‡½æ•°å¿…é¡»åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰ï¼Œå¦åˆ™Windowsçš„spawnæ¨¡å¼æ— æ³•pickle
    - å†…éƒ¨çš„_initå‡½æ•°è™½ç„¶æ˜¯é—­åŒ…ï¼Œä½†ç”±äºå¤–å±‚å‡½æ•°åœ¨æ¨¡å—é¡¶å±‚ï¼Œæ•´ä½“å¯pickle
    
    Args:
        sim_config: ä»¿çœŸé…ç½®
        rl_config: RLé…ç½®
        log_dir: æ—¥å¿—ç›®å½•
        rank: ç¯å¢ƒç¼–å·ï¼ˆç”¨äºåŒºåˆ†å¤šç¯å¢ƒï¼‰
    
    Returns:
        ç¯å¢ƒåˆ›å»ºå‡½æ•°
    """
    def _init():
        env = DeliveryRLEnvironment(
            simulation_config=sim_config,
            rl_config=rl_config
        )
        # ä½¿ç”¨MonitoråŒ…è£…ä»¥è®°å½•episodeä¿¡æ¯
        monitor_path = f"{log_dir}/env_{rank}" if log_dir else None
        return Monitor(env, filename=monitor_path, allow_early_resets=True)
    return _init


# ============================================================
# è‡ªå®šä¹‰å›è°ƒå‡½æ•°
# ============================================================

class TrainingMonitorCallback(BaseCallback):
    """
    è®­ç»ƒç›‘æ§å›è°ƒ
    
    åŠŸèƒ½ï¼š
    1. å®šæœŸè®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoard
    2. æ‰“å°è®­ç»ƒè¿›åº¦
    3. æ—©åœæ£€æµ‹
    4. ä¿å­˜è®­ç»ƒå†å²
    """
    
    def __init__(self, 
                 check_freq: int = 1000,
                 log_dir: Optional[str] = None,
                 early_stop_patience: int = 10,
                 min_improvement: float = 0.01,
                 verbose: int = 1):
        """
        åˆå§‹åŒ–è®­ç»ƒç›‘æ§å›è°ƒ
        
        Args:
            check_freq: æ£€æŸ¥é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
            log_dir: æ—¥å¿—ç›®å½•
            early_stop_patience: æ—©åœè€å¿ƒå€¼ï¼ˆè¯„ä¼°æ¬¡æ•°ï¼‰
            min_improvement: æœ€å°æ”¹è¿›é˜ˆå€¼
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.check_freq = check_freq
        self.log_dir = Path(log_dir) if log_dir else None
        self.early_stop_patience = early_stop_patience
        self.min_improvement = min_improvement
        
        # è®­ç»ƒå†å²
        self.history = {
            'timesteps': [],
            'episode_rewards': [],
            'episode_lengths': [],
            'mean_rewards': [],
            'std_rewards': [],
            'completion_rates': [],
            'timeout_rates': []
        }
        
        # æ—©åœç›¸å…³
        self.best_mean_reward = -np.inf
        self.no_improvement_count = 0
        self.should_stop = False
        
        # æ—¶é—´ç»Ÿè®¡
        self.start_time = None
        self.last_check_time = None
    
    def _on_training_start(self) -> None:
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
        self.start_time = time.time()
        self.last_check_time = self.start_time
        logger.info("="*60)
        logger.info("PPOè®­ç»ƒå¼€å§‹")
        logger.info("="*60)
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        if self.n_calls % self.check_freq == 0:
            self._log_progress()
        
        return not self.should_stop
    
    def _log_progress(self) -> None:
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        current_time = time.time()
        elapsed = current_time - self.start_time
        interval = current_time - self.last_check_time
        self.last_check_time = current_time
        
        # è·å–æœ€è¿‘çš„episodeä¿¡æ¯
        if len(self.model.ep_info_buffer) > 0:
            ep_rewards = [ep['r'] for ep in self.model.ep_info_buffer]
            ep_lengths = [ep['l'] for ep in self.model.ep_info_buffer]
            
            mean_reward = np.mean(ep_rewards)
            std_reward = np.std(ep_rewards)
            mean_length = np.mean(ep_lengths)
            
            # è®°å½•å†å²
            self.history['timesteps'].append(self.num_timesteps)
            self.history['mean_rewards'].append(mean_reward)
            self.history['std_rewards'].append(std_reward)
            
            # è®¡ç®—FPS
            fps = self.check_freq / max(interval, 0.001)
            
            # æ‰“å°è¿›åº¦
            if self.verbose > 0:
                logger.info(
                    f"[Step {self.num_timesteps:,}] "
                    f"Reward: {mean_reward:.2f}Â±{std_reward:.2f} | "
                    f"EpLen: {mean_length:.0f} | "
                    f"FPS: {fps:.0f} | "
                    f"Time: {elapsed/60:.1f}min"
                )
            
            # TensorBoardè®°å½•
            if self.logger:
                self.logger.record('train/mean_reward', mean_reward)
                self.logger.record('train/std_reward', std_reward)
                self.logger.record('train/mean_ep_length', mean_length)
                self.logger.record('time/fps', fps)
                self.logger.record('time/elapsed_minutes', elapsed / 60)
            
            # è®°å½•ä¸šåŠ¡æŒ‡æ ‡åˆ°TensorBoardï¼ˆä»ç¯å¢ƒçš„episode_statsä¸­æå–ï¼‰
            # æ³¨æ„ï¼šVecEnvä¸‹éœ€è¦ä»infosä¸­è·å–ï¼Œè¿™é‡Œå°è¯•ä»æœ€è¿‘çš„episodeä¸­æå–
            try:
                infos = self.locals.get('infos', [])
                for info in infos:
                    if 'episode_stats' in info:
                        stats = info['episode_stats']
                        completion_rate = stats.get('completion_rate', 0)
                        timeout_rate = stats.get('timeout_rate', 0)
                        avg_service_time = stats.get('avg_service_time', 0)
                        
                        # è®°å½•åˆ°å†å²
                        self.history['completion_rates'].append(completion_rate)
                        self.history['timeout_rates'].append(timeout_rate)
                        
                        # è®°å½•åˆ°TensorBoard
                        if self.logger:
                            self.logger.record('business/completion_rate', completion_rate)
                            self.logger.record('business/timeout_rate', timeout_rate)
                            self.logger.record('business/avg_service_time', avg_service_time)
                        break  # åªè®°å½•ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„episode_stats
            except Exception as e:
                pass  # é™é»˜å¤„ç†ï¼Œä¸å½±å“è®­ç»ƒ
            
            # æ›´æ–°æœ€ä½³å¥–åŠ±è®°å½•ï¼ˆä¸æ—©åœé€»è¾‘åˆ†ç¦»ï¼‰
            self._update_best_mean_reward(mean_reward)
            
            # æ—©åœæ£€æŸ¥ï¼ˆå·²ç¦ç”¨ï¼Œæ”¹ç”¨è¯¾ç¨‹å­¦ä¹ çš„è¾¾æ ‡è·³è½¬æœºåˆ¶ï¼‰
            # self._check_early_stop(mean_reward)
    
    def _update_best_mean_reward(self, mean_reward: float) -> None:
        """æ›´æ–°æœ€ä½³å¹³å‡å¥–åŠ±è®°å½•ï¼ˆä¸æ—©åœé€»è¾‘åˆ†ç¦»ï¼‰"""
        if mean_reward > self.best_mean_reward:
            self.best_mean_reward = mean_reward
    
    def _check_early_stop(self, mean_reward: float) -> None:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        if mean_reward > self.best_mean_reward + self.min_improvement:
            self.best_mean_reward = mean_reward
            self.no_improvement_count = 0
        else:
            self.no_improvement_count += 1
        
        if self.no_improvement_count >= self.early_stop_patience:
            logger.warning(
                f"æ—©åœè§¦å‘ï¼è¿ç»­{self.early_stop_patience}æ¬¡è¯„ä¼°æ— æ”¹è¿› "
                f"(æœ€ä½³å¥–åŠ±: {self.best_mean_reward:.2f})"
            )
            self.should_stop = True
    
    def _on_training_end(self) -> None:
        """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨"""
        total_time = time.time() - self.start_time
        logger.info("="*60)
        logger.info("PPOè®­ç»ƒç»“æŸ")
        logger.info(f"  æ€»æ­¥æ•°: {self.num_timesteps:,}")
        logger.info(f"  æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
        logger.info(f"  æœ€ä½³å¹³å‡å¥–åŠ±: {self.best_mean_reward:.2f}")
        logger.info("="*60)
        
        # ä¿å­˜è®­ç»ƒå†å²
        if self.log_dir:
            history_path = self.log_dir / 'training_history.json'
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, indent=2)
            logger.info(f"è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


class EpisodeMetricsCallback(BaseCallback):
    """
    EpisodeæŒ‡æ ‡å›è°ƒï¼ˆå¯é€‰ï¼Œç›®å‰æœªé»˜è®¤å¯ç”¨ï¼‰
    
    è®°å½•æ¯ä¸ªEpisodeçš„è¯¦ç»†æŒ‡æ ‡ï¼ˆå®Œæˆç‡ã€è¶…æ—¶ç‡ç­‰ï¼‰
    
    ä½¿ç”¨æ–¹å¼ï¼šåœ¨_create_callbacksä¸­æ‰‹åŠ¨æ·»åŠ ï¼Œæˆ–åœ¨è‡ªå®šä¹‰è®­ç»ƒæµç¨‹ä¸­ä½¿ç”¨ï¼š
    ```python
    eval_env = trainer.create_env()
    metrics_callback = EpisodeMetricsCallback(
        eval_env=eval_env,
        eval_freq=5000,
        n_eval_episodes=5
    )
    callbacks.append(metrics_callback)
    ```
    """
    
    def __init__(self, 
                 eval_env: Any = None,
                 eval_freq: int = 5000,
                 n_eval_episodes: int = 5,
                 verbose: int = 1):
        """
        åˆå§‹åŒ–EpisodeæŒ‡æ ‡å›è°ƒ
        
        Args:
            eval_env: è¯„ä¼°ç¯å¢ƒ
            eval_freq: è¯„ä¼°é¢‘ç‡
            n_eval_episodes: æ¯æ¬¡è¯„ä¼°çš„Episodeæ•°
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        
        # è¯„ä¼°å†å²
        self.eval_history = {
            'timesteps': [],
            'completion_rates': [],
            'timeout_rates': [],
            'mean_rewards': []
        }
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        if self.eval_env is not None and self.n_calls % self.eval_freq == 0:
            self._evaluate()
        return True
    
    def _evaluate(self) -> None:
        """æ‰§è¡Œè¯„ä¼°"""
        if self.verbose > 0:
            logger.info(f"[Step {self.num_timesteps}] æ‰§è¡Œè¯„ä¼°...")
        
        completion_rates = []
        timeout_rates = []
        rewards = []
        
        for ep in range(self.n_eval_episodes):
            obs, info = self.eval_env.reset()
            done = False
            episode_reward = 0
            
            while not done:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.eval_env.step(action)
                episode_reward += reward
                done = terminated or truncated
            
            # æ”¶é›†Episodeç»Ÿè®¡
            stats = self.eval_env.get_episode_statistics()
            completion_rates.append(stats.get('completion_rate', 0))
            timeout_rates.append(stats.get('timeout_rate', 0))
            rewards.append(episode_reward)
        
        # è®¡ç®—å¹³å‡å€¼
        mean_completion = np.mean(completion_rates)
        mean_timeout = np.mean(timeout_rates)
        mean_reward = np.mean(rewards)
        
        # è®°å½•å†å²
        self.eval_history['timesteps'].append(self.num_timesteps)
        self.eval_history['completion_rates'].append(mean_completion)
        self.eval_history['timeout_rates'].append(mean_timeout)
        self.eval_history['mean_rewards'].append(mean_reward)
        
        # TensorBoardè®°å½•
        if self.logger:
            self.logger.record('eval/completion_rate', mean_completion)
            self.logger.record('eval/timeout_rate', mean_timeout)
            self.logger.record('eval/mean_reward', mean_reward)
        
        if self.verbose > 0:
            logger.info(
                f"  è¯„ä¼°ç»“æœ: å®Œæˆç‡={mean_completion:.1%}, "
                f"è¶…æ—¶ç‡={mean_timeout:.1%}, å¥–åŠ±={mean_reward:.2f}"
            )


# ============================================================
# è¯¾ç¨‹è¾¾æ ‡è·³è½¬å›è°ƒ
# ============================================================

class CurriculumAdvanceCallback(BaseCallback):
    """
    è¯¾ç¨‹è¾¾æ ‡è·³è½¬å›è°ƒ
    
    å½“æ¨¡å‹æ€§èƒ½è¾¾åˆ°å½“å‰é˜¶æ®µçš„é˜ˆå€¼æ—¶ï¼Œæå‰ç»“æŸå½“å‰é˜¶æ®µï¼Œè¿›å…¥ä¸‹ä¸€é˜¶æ®µ
    """
    
    def __init__(self,
                 eval_env: Any,
                 min_completion_rate: float = 0.5,
                 max_timeout_rate: float = 0.5,
                 eval_freq: int = 5000,
                 n_eval_episodes: int = 3,
                 min_timesteps: int = 10000,
                 verbose: int = 1):
        """
        åˆå§‹åŒ–è¯¾ç¨‹è¾¾æ ‡è·³è½¬å›è°ƒ
        
        Args:
            eval_env: è¯„ä¼°ç¯å¢ƒ
            min_completion_rate: æœ€å°å®Œæˆç‡é˜ˆå€¼ï¼ˆè¾¾åˆ°æ­¤å€¼æ‰èƒ½è·³è½¬ï¼‰
            max_timeout_rate: æœ€å¤§è¶…æ—¶ç‡é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼æ‰èƒ½è·³è½¬ï¼‰
            eval_freq: è¯„ä¼°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
            n_eval_episodes: æ¯æ¬¡è¯„ä¼°çš„Episodeæ•°
            min_timesteps: æœ€å°‘è®­ç»ƒæ­¥æ•°ï¼ˆé˜²æ­¢è¿‡æ—©è·³è½¬ï¼‰
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.eval_env = eval_env
        self.min_completion_rate = min_completion_rate
        self.max_timeout_rate = max_timeout_rate
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.min_timesteps = min_timesteps
        
        # è¾¾æ ‡çŠ¶æ€
        self.stage_completed = False
        self.completion_reason = ""
        self.best_completion_rate = 0.0
        self.best_timeout_rate = 1.0
        
        # è¯„ä¼°å†å²
        self.eval_history = []
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨ï¼Œè¿”å›Falseä¼šåœæ­¢è®­ç»ƒ"""
        # å®šæœŸè¯„ä¼°
        if self.n_calls % self.eval_freq == 0 and self.num_timesteps >= self.min_timesteps:
            should_advance = self._evaluate_and_check()
            if should_advance:
                return False  # åœæ­¢å½“å‰é˜¶æ®µè®­ç»ƒ
        return True
    
    def _evaluate_and_check(self) -> bool:
        """
        è¯„ä¼°æ¨¡å‹å¹¶æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
        
        Returns:
            æ˜¯å¦åº”è¯¥è·³è½¬åˆ°ä¸‹ä¸€é˜¶æ®µ
        """
        completion_rates = []
        timeout_rates = []
        
        for ep in range(self.n_eval_episodes):
            obs, info = self.eval_env.reset()
            done = False
            
            while not done:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.eval_env.step(action)
                done = terminated or truncated
            
            # æ”¶é›†ç»Ÿè®¡
            stats = self.eval_env.get_episode_statistics()
            completion_rates.append(stats.get('completion_rate', 0))
            timeout_rates.append(stats.get('timeout_rate', 1))
        
        # è®¡ç®—å¹³å‡å€¼
        mean_completion = np.mean(completion_rates)
        mean_timeout = np.mean(timeout_rates)
        
        # è®°å½•å†å²
        self.eval_history.append({
            'timesteps': self.num_timesteps,
            'completion_rate': mean_completion,
            'timeout_rate': mean_timeout
        })
        
        # æ›´æ–°æœ€ä½³è®°å½•
        if mean_completion > self.best_completion_rate:
            self.best_completion_rate = mean_completion
        if mean_timeout < self.best_timeout_rate:
            self.best_timeout_rate = mean_timeout
        
        # TensorBoardè®°å½•
        if self.logger:
            self.logger.record('curriculum/completion_rate', mean_completion)
            self.logger.record('curriculum/timeout_rate', mean_timeout)
            self.logger.record('curriculum/target_completion', self.min_completion_rate)
            self.logger.record('curriculum/target_timeout', self.max_timeout_rate)
        
        if self.verbose > 0:
            logger.info(
                f"[Step {self.num_timesteps:,}] è¯¾ç¨‹è¯„ä¼°: "
                f"å®Œæˆç‡={mean_completion:.1%} (ç›®æ ‡â‰¥{self.min_completion_rate:.1%}), "
                f"è¶…æ—¶ç‡={mean_timeout:.1%} (ç›®æ ‡â‰¤{self.max_timeout_rate:.1%})"
            )
        
        # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
        if mean_completion >= self.min_completion_rate and mean_timeout <= self.max_timeout_rate:
            self.stage_completed = True
            self.completion_reason = (
                f"è¾¾æ ‡è·³è½¬ï¼å®Œæˆç‡={mean_completion:.1%}â‰¥{self.min_completion_rate:.1%}, "
                f"è¶…æ—¶ç‡={mean_timeout:.1%}â‰¤{self.max_timeout_rate:.1%}"
            )
            logger.info(f"ğŸ‰ {self.completion_reason}")
            return True
        
        return False


# ============================================================
# ç®€åŒ–ç‰ˆè¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ï¼ˆä»…ç”¨äºtrain_with_curriculumå†…éƒ¨ï¼‰
# æ³¨æ„ï¼šDay 14çš„å®Œæ•´ç‰ˆCurriculumManageråœ¨curriculum_learning.pyä¸­
# ============================================================

class SimpleCurriculumManager:
    """
    ç®€åŒ–ç‰ˆè¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ï¼ˆä»…ç”¨äºtrain_with_curriculumå†…éƒ¨ï¼‰
    
    ç®¡ç†ä»ä½è´Ÿè½½åˆ°é«˜è´Ÿè½½çš„è®­ç»ƒè¯¾ç¨‹ã€‚
    å¦‚éœ€æ›´å®Œæ•´çš„åŠŸèƒ½ï¼ˆå›é€€ã€å¹³æ»‘è¿‡æ¸¡ç­‰ï¼‰ï¼Œè¯·ä½¿ç”¨curriculum_learning.pyä¸­çš„CurriculumManagerã€‚
    """
    
    def __init__(self, stages: List[Dict[str, Any]]):
        """
        åˆå§‹åŒ–è¯¾ç¨‹ç®¡ç†å™¨
        
        Args:
            stages: è¯¾ç¨‹é˜¶æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªé˜¶æ®µåŒ…å«é…ç½®å’Œè®­ç»ƒæ­¥æ•°
        """
        self.stages = stages
        self.current_stage_idx = 0
        self.total_timesteps_trained = 0
    
    @property
    def current_stage(self) -> Dict[str, Any]:
        """è·å–å½“å‰é˜¶æ®µ"""
        if self.current_stage_idx < len(self.stages):
            return self.stages[self.current_stage_idx]
        return self.stages[-1]
    
    @property
    def is_completed(self) -> bool:
        """æ˜¯å¦å®Œæˆæ‰€æœ‰é˜¶æ®µ"""
        return self.current_stage_idx >= len(self.stages)
    
    def advance(self) -> bool:
        """
        è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
        
        Returns:
            æ˜¯å¦è¿˜æœ‰æ›´å¤šé˜¶æ®µ
        """
        self.current_stage_idx += 1
        if self.current_stage_idx < len(self.stages):
            logger.info(f"è¿›å…¥è¯¾ç¨‹é˜¶æ®µ {self.current_stage_idx + 1}/{len(self.stages)}")
            logger.info(f"  åç§°: {self.current_stage['name']}")
            logger.info(f"  è®¢å•æ•°: {self.current_stage['total_orders']}")
            logger.info(f"  è®­ç»ƒæ­¥æ•°: {self.current_stage['timesteps']:,}")
            return True
        return False
    
    def get_stage_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰é˜¶æ®µçš„ç¯å¢ƒé…ç½®"""
        stage = self.current_stage
        return {
            'total_orders': stage.get('total_orders', 500),
            'num_couriers': stage.get('num_couriers', 20),
            'simulation_duration': stage.get('simulation_duration', 7200)
        }


# ============================================================
# ä¸»è®­ç»ƒå™¨ç±»
# ============================================================

class RLTrainer:
    """
    PPOè®­ç»ƒæµæ°´çº¿
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½é…ç½®å¹¶åˆ›å»ºç¯å¢ƒ
    2. é…ç½®TensorBoardç›‘æ§
    3. æ”¯æŒè¯¾ç¨‹å­¦ä¹ 
    4. æ¨¡å‹ä¿å­˜ä¸æ¢å¤
    5. è®­ç»ƒåè¯„ä¼°
    """
    
    def __init__(self, config_path: str, scenario: str = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: RLé…ç½®æ–‡ä»¶è·¯å¾„
            scenario: ä½¿ç”¨çš„åœºæ™¯åç§°ï¼ˆå¯é€‰ï¼Œè¦†ç›–é»˜è®¤é…ç½®ï¼‰
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.scenario = scenario
        
        # æå–é…ç½®
        self.sim_config = self.config.get('simulation', {})
        self.rl_config = self.config.get('rl', {})
        self.training_config = self.rl_config.get('training', {})
        self.scenarios_config = self.config.get('scenarios', {})
        
        # å¦‚æœæŒ‡å®šäº†åœºæ™¯ï¼Œä½¿ç”¨åœºæ™¯é…ç½®è¦†ç›–é»˜è®¤é…ç½®
        if scenario and scenario in self.scenarios_config:
            scenario_config = self.scenarios_config[scenario]
            self.sim_config.update(scenario_config)
            logger.info(f"ä½¿ç”¨åœºæ™¯é…ç½®: {scenario}")
        
        # è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        scenario_suffix = f"_{scenario}" if scenario else ""
        self.output_dir = Path(self.rl_config.get('model_save_path', './outputs/rl_training')) / f"{timestamp}{scenario_suffix}"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoardæ—¥å¿—ç›®å½•
        self.tensorboard_dir = self.output_dir / 'tensorboard'
        self.tensorboard_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä½¿ç”¨çš„é…ç½®
        self._save_config()
        
        logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"  TensorBoardæ—¥å¿—: {self.tensorboard_dir}")
    
    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"é…ç½®æ–‡ä»¶å·²åŠ è½½: {self.config_path}")
        return config
    
    def _save_config(self) -> None:
        """ä¿å­˜å½“å‰ä½¿ç”¨çš„é…ç½®åˆ°è¾“å‡ºç›®å½•"""
        config_save_path = self.output_dir / 'config_used.yaml'
        with open(config_save_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config, f, allow_unicode=True, default_flow_style=False)
        logger.info(f"é…ç½®å·²ä¿å­˜: {config_save_path}")
    
    def create_env(self, **kwargs):
        """
        åˆ›å»ºRLç¯å¢ƒ
        
        Returns:
            RLç¯å¢ƒå®ä¾‹
        """
        # åˆå¹¶é…ç½®
        sim_config = {**self.sim_config, **kwargs}
        
        env = DeliveryRLEnvironment(
            simulation_config=sim_config,
            rl_config=self.rl_config
        )
        
        # æ·»åŠ MonitoråŒ…è£…å™¨ï¼ˆå…³é”®ä¿®å¤ï¼šç”¨äºè®°å½•Episodeç»Ÿè®¡ï¼‰
        # æ²¡æœ‰Monitorï¼Œep_info_bufferå°†ä¸ºç©ºï¼Œå¯¼è‡´æ— æ³•è®¡ç®—å¹³å‡å¥–åŠ±
        if SB3_AVAILABLE:
            env = Monitor(env, filename=None, allow_early_resets=True)
        
        return env
    
    def train(self, force_single_scenario: bool = False):
        """
        æ‰§è¡Œè®­ç»ƒæµç¨‹
        
        Args:
            force_single_scenario: å¼ºåˆ¶ä½¿ç”¨å•åœºæ™¯è®­ç»ƒï¼Œå¿½ç•¥é…ç½®ä¸­çš„use_curriculum_learning
        """
        if not SB3_AVAILABLE:
            logger.error("Stable-Baselines3æœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒ")
            return
        
        # æ£€æŸ¥é…ç½®æ˜¯å¦å¯ç”¨è¯¾ç¨‹å­¦ä¹ ï¼ˆé™¤éå¼ºåˆ¶å•åœºæ™¯ï¼‰
        training_strategy = self.training_config.get('training_strategy', {})
        use_curriculum = training_strategy.get('use_curriculum_learning', False)
        
        if use_curriculum and not force_single_scenario:
            logger.info("é…ç½®å¯ç”¨äº†è¯¾ç¨‹å­¦ä¹ ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° train_with_curriculum()")
            return self.train_with_curriculum()
        
        logger.info("="*60)
        logger.info("å¼€å§‹RLè®­ç»ƒï¼ˆå•åœºæ™¯æ¨¡å¼ï¼‰")
        logger.info("="*60)
        
        # 1. åˆ›å»ºç¯å¢ƒ
        logger.info("\næ­¥éª¤1: åˆ›å»ºè®­ç»ƒç¯å¢ƒ")
        
        # ä¼˜å…ˆä» training_strategy.num_parallel_envs è¯»å–ï¼Œå…¼å®¹æ—§é…ç½®
        training_strategy = self.training_config.get('training_strategy', {})
        num_parallel_envs = training_strategy.get('num_parallel_envs', 
                                                   self.training_config.get('num_parallel_envs', 1))
        
        if num_parallel_envs > 1:
            # å¹¶è¡Œç¯å¢ƒ
            # ä½¿ç”¨æ¨¡å—çº§å·¥å‚å‡½æ•°ï¼Œç¡®ä¿Windowsçš„spawnæ¨¡å¼å¯ä»¥pickle
            import platform
            log_dir_str = str(self.output_dir) if self.output_dir else None
            env_fns = [
                _make_env_factory(self.sim_config, self.rl_config, log_dir_str, rank=i)
                for i in range(num_parallel_envs)
            ]
            
            # Windowsä¸ŠSubprocVecEnvå¯èƒ½ä»æœ‰é—®é¢˜ï¼Œæä¾›DummyVecEnvå›é€€
            if platform.system() == 'Windows':
                try:
                    env = SubprocVecEnv(env_fns)
                    logger.info(f"  åˆ›å»ºäº† {num_parallel_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ (SubprocVecEnv)")
                except Exception as e:
                    logger.warning(f"SubprocVecEnvåˆ›å»ºå¤±è´¥: {e}ï¼Œå›é€€åˆ°DummyVecEnv")
                    env = DummyVecEnv(env_fns)
                    logger.info(f"  åˆ›å»ºäº† {num_parallel_envs} ä¸ªç¯å¢ƒ (DummyVecEnvå›é€€)")
            else:
                env = SubprocVecEnv(env_fns)
                logger.info(f"  åˆ›å»ºäº† {num_parallel_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ (SubprocVecEnv)")
        else:
            # å•ç¯å¢ƒ
            env = DummyVecEnv([self.create_env])
            logger.info("  åˆ›å»ºäº†å•ä¸ªè®­ç»ƒç¯å¢ƒ")
        
        # 2. æ£€æŸ¥ç¯å¢ƒï¼ˆè°ƒè¯•ç”¨ï¼‰
        logger.info("\næ­¥éª¤2: æ£€æŸ¥ç¯å¢ƒå…¼å®¹æ€§")
        try:
            single_env = self.create_env()
            check_env(single_env)
            logger.info("  âœ“ ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
            single_env.close()
        except Exception as e:
            logger.warning(f"  ç¯å¢ƒæ£€æŸ¥è­¦å‘Š: {str(e)}")
        
        # 3. åˆ›å»ºPPOæ¨¡å‹
        logger.info("\næ­¥éª¤3: åˆå§‹åŒ–PPOç®—æ³•")
        model = self._create_ppo_model(env)
        
        logger.info("  ç®—æ³•: PPO")
        logger.info(f"  ç­–ç•¥ç½‘ç»œ: {self.training_config.get('policy', {}).get('net_arch')}")
        
        # é…ç½®TensorBoard loggerï¼ˆä¸è¯¾ç¨‹å­¦ä¹ ä¸€è‡´ï¼‰
        new_logger = configure(str(self.tensorboard_dir), ["stdout", "tensorboard"])
        model.set_logger(new_logger)
        logger.info(f"  TensorBoardæ—¥å¿—: {self.tensorboard_dir}")
        
        # 4. é…ç½®å›è°ƒ
        logger.info("\næ­¥éª¤4: é…ç½®è®­ç»ƒå›è°ƒ")
        callbacks = self._create_callbacks(env)
        
        # 5. å¼€å§‹è®­ç»ƒ
        logger.info("\næ­¥éª¤5: å¼€å§‹è®­ç»ƒ")
        total_timesteps = self.training_config.get('total_timesteps', 1000000)
        logger.info(f"  æ€»æ­¥æ•°: {total_timesteps:,}")
        
        try:
            model.learn(
                total_timesteps=total_timesteps,
                callback=callbacks,
                progress_bar=True
            )
            logger.info("\nè®­ç»ƒå®Œæˆï¼")
            
            # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = self.output_dir / "final_model"
            model.save(final_model_path)
            logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
            
        except KeyboardInterrupt:
            logger.info("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            interrupt_model_path = self.output_dir / "interrupted_model"
            model.save(interrupt_model_path)
            logger.info(f"ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupt_model_path}")
        
        # 7. å…³é—­ç¯å¢ƒ
        env.close()
        logger.info("ç¯å¢ƒå·²å…³é—­")
    
    def _create_ppo_model(self, env):
        """åˆ›å»ºPPOæ¨¡å‹"""
        ppo_config = self.training_config.get('ppo', {})
        policy_config = self.training_config.get('policy', {})
        
        model = PPO(
            policy="MlpPolicy",
            env=env,
            learning_rate=self.training_config.get('learning_rate', 3e-4),
            n_steps=ppo_config.get('n_steps', 2048),
            batch_size=ppo_config.get('batch_size', 64),
            n_epochs=ppo_config.get('n_epochs', 10),
            gamma=self.training_config.get('gamma', 0.99),
            gae_lambda=ppo_config.get('gae_lambda', 0.95),
            clip_range=ppo_config.get('clip_range', 0.2),
            ent_coef=ppo_config.get('ent_coef', 0.01),
            vf_coef=ppo_config.get('vf_coef', 0.5),
            max_grad_norm=ppo_config.get('max_grad_norm', 0.5),
            normalize_advantage=ppo_config.get('normalize_advantage', True),
            policy_kwargs=dict(
                net_arch=policy_config.get('net_arch', [256, 256])
            ),
            verbose=1,
            seed=self.rl_config.get('seed', self.config.get('seed', 42))
        )
        
        return model
    
    
    def _create_callbacks(self, env):
        """åˆ›å»ºè®­ç»ƒå›è°ƒ"""
        callbacks = []
        
        eval_config = self.rl_config.get('evaluation', {})
        
        # 1. è®­ç»ƒç›‘æ§å›è°ƒï¼ˆè‡ªå®šä¹‰ï¼‰
        monitor_callback = TrainingMonitorCallback(
            check_freq=1000,
            log_dir=str(self.output_dir),
            early_stop_patience=150,  # 150æ¬¡æ£€æŸ¥æ— æ”¹è¿›åˆ™åœæ­¢ï¼ˆçº¦150kæ­¥ï¼‰
            min_improvement=10.0,  # å¥–åŠ±å€¼åœ¨2000+èŒƒå›´ï¼Œæé«˜é˜ˆå€¼é¿å…è¿‡æ—©åœæ­¢
            verbose=1
        )
        callbacks.append(monitor_callback)
        logger.info("  âœ“ è®­ç»ƒç›‘æ§å›è°ƒï¼ˆæ¯1000æ­¥ï¼‰")
        
        # 2. æ ‡å‡†è¯„ä¼°å›è°ƒ
        if eval_config.get('eval_freq', 0) > 0:
            eval_env = DummyVecEnv([self.create_env])
            
            eval_callback = EvalCallback(
                eval_env,
                best_model_save_path=str(self.output_dir / "best_model"),
                log_path=str(self.output_dir / "eval_logs"),
                eval_freq=eval_config.get('eval_freq', 10000),
                n_eval_episodes=eval_config.get('n_eval_episodes', 10),
                deterministic=True,
                render=False
            )
            callbacks.append(eval_callback)
            logger.info(f"  âœ“ è¯„ä¼°å›è°ƒï¼ˆæ¯ {eval_config['eval_freq']} æ­¥ï¼‰")
        
        # 3. æ£€æŸ¥ç‚¹å›è°ƒ
        if eval_config.get('save_freq', 0) > 0:
            checkpoint_callback = CheckpointCallback(
                save_freq=eval_config.get('save_freq', 50000),
                save_path=str(self.output_dir / "checkpoints"),
                name_prefix="rl_model"
            )
            callbacks.append(checkpoint_callback)
            logger.info(f"  âœ“ æ£€æŸ¥ç‚¹å›è°ƒï¼ˆæ¯ {eval_config['save_freq']} æ­¥ï¼‰")
        
        return CallbackList(callbacks) if callbacks else None
    
    def train_with_curriculum(self):
        """
        ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼ˆè¾¾æ ‡å³è·³è½¬æ¨¡å¼ï¼‰
        
        ä»ä½è´Ÿè½½åœºæ™¯å¼€å§‹ï¼Œé€æ­¥å¢åŠ éš¾åº¦ã€‚
        å½“æ¨¡å‹æ€§èƒ½è¾¾åˆ°å½“å‰é˜¶æ®µçš„é˜ˆå€¼æ—¶ï¼Œç«‹å³è·³è½¬åˆ°ä¸‹ä¸€é˜¶æ®µã€‚
        
        æ³¨æ„ï¼šæœ¬æ–¹æ³•æ˜¯ç®€åŒ–ç‰ˆè¯¾ç¨‹å­¦ä¹ å®ç°ï¼Œç›´æ¥è¯»å–configä¸­çš„curriculum_stagesã€‚
        å¦‚éœ€æ›´å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ åŠŸèƒ½ï¼ˆå›é€€æœºåˆ¶ã€å¹³æ»‘è¿‡æ¸¡ã€éš¾åº¦ç»´åº¦ç­‰ï¼‰ï¼Œ
        è¯·ä½¿ç”¨ src/rl/train_with_curriculum.py ä¸­çš„ CurriculumTrainerï¼Œ
        å®ƒåŸºäº src/rl/curriculum_learning.py ä¸­çš„ CurriculumManagerã€‚
        
        ä¸¤å¥—å®ç°çš„åŒºåˆ«ï¼š
        - æœ¬æ–¹æ³•ï¼šç®€å•ã€ç›´æ¥è¯»å–YAMLé…ç½®ã€æ”¯æŒè¾¾æ ‡è·³è½¬å’ŒåŠ æ—¶èµ›
        - CurriculumTrainerï¼šå®Œæ•´æ¡†æ¶ã€æ”¯æŒå›é€€/å¹³æ»‘è¿‡æ¸¡/éš¾åº¦è¯„åˆ†ç­‰é«˜çº§åŠŸèƒ½
        """
        if not SB3_AVAILABLE:
            raise RuntimeError("Stable-Baselines3æœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒ")
        
        # è¯¾ç¨‹å­¦ä¹ ä½¿ç”¨PPOç®—æ³•
        
        # è·å–è¯¾ç¨‹å­¦ä¹ é…ç½®
        curriculum_config = self.training_config.get('curriculum', {})
        stages = curriculum_config.get('curriculum_stages', [])
        
        if not stages:
            logger.warning("æœªé…ç½®è¯¾ç¨‹å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨é»˜è®¤è®­ç»ƒ")
            return self.train()
        
        logger.info("="*60)
        logger.info("å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒï¼ˆè¾¾æ ‡å³è·³è½¬æ¨¡å¼ï¼‰")
        logger.info(f"å…± {len(stages)} ä¸ªé˜¶æ®µ")
        logger.info("="*60)
        
        # åˆ›å»ºåˆå§‹æ¨¡å‹
        model = None
        total_stages_completed = 0
        current_env = None  # è·Ÿè¸ªå½“å‰ç¯å¢ƒï¼Œç”¨äºé˜¶æ®µåˆ‡æ¢æ—¶å…³é—­
        
        for stage_idx, stage in enumerate(stages):
            # æ‰“å°é˜¶æ®µä¿¡æ¯
            logger.info(f"\n{'='*60}")
            logger.info(f"è¯¾ç¨‹é˜¶æ®µ {stage_idx + 1}/{len(stages)}: {stage['name']}")
            logger.info(f"{'='*60}")
            logger.info(f"  æè¿°: {stage.get('description', 'N/A')}")
            logger.info(f"  è®¢å•æ–‡ä»¶: {stage.get('orders_file', self.sim_config.get('orders_file'))}")
            logger.info(f"  è®¢å•æ•°: {stage['total_orders']}")
            logger.info(f"  éª‘æ‰‹æ•°: {stage.get('num_couriers', 20)}")
            logger.info(f"  æœ€å¤§è®­ç»ƒæ­¥æ•°: {stage['timesteps']:,}")
            logger.info(f"  è¾¾æ ‡æ¡ä»¶: å®Œæˆç‡â‰¥{stage.get('min_completion_rate', 0.5):.0%}, "
                       f"è¶…æ—¶ç‡â‰¤{stage.get('max_timeout_rate', 0.5):.0%}")
            
            # åˆ›å»ºè¯¥é˜¶æ®µçš„ç¯å¢ƒé…ç½®
            stage_num_couriers = stage.get('num_couriers', 20)
            
            # éªŒè¯éª‘æ‰‹æ•°ä¸è¶…è¿‡max_couriersï¼ˆåŠ¨ä½œç©ºé—´ä¸Šé™ï¼‰
            # è¿™æ˜¯è¯¾ç¨‹å­¦ä¹ çš„å…³é”®çº¦æŸï¼šæ‰€æœ‰é˜¶æ®µå¿…é¡»ä½¿ç”¨ç›¸åŒçš„åŠ¨ä½œç©ºé—´ç»´åº¦
            max_couriers = self.rl_config.get('state_encoder', {}).get('max_couriers', 50)
            if stage_num_couriers > max_couriers:
                raise ValueError(
                    f"è¯¾ç¨‹é˜¶æ®µ'{stage['name']}'çš„num_couriers={stage_num_couriers}è¶…è¿‡äº†"
                    f"state_encoder.max_couriers={max_couriers}ã€‚"
                    f"è¯·å‡å°‘è¯¥é˜¶æ®µçš„éª‘æ‰‹æ•°ï¼Œæˆ–å¢å¤§max_couriersé…ç½®ã€‚"
                )
            
            stage_sim_config = {
                **self.sim_config,
                'total_orders': stage['total_orders'],
                'num_couriers': stage_num_couriers,
                'simulation_duration': stage.get('simulation_duration', self.sim_config.get('simulation_duration', 43200)),
                # ä½¿ç”¨é˜¶æ®µç‰¹å®šçš„è®¢å•æ–‡ä»¶ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
                'orders_file': stage.get('orders_file', self.sim_config.get('orders_file'))
            }
            
            # å…³é—­ä¸Šä¸€é˜¶æ®µçš„ç¯å¢ƒï¼ˆé¿å…SubprocVecEnvå­è¿›ç¨‹æ³„éœ²ï¼‰
            if current_env is not None:
                try:
                    current_env.close()
                    logger.debug("å·²å…³é—­ä¸Šä¸€é˜¶æ®µçš„è®­ç»ƒç¯å¢ƒ")
                except Exception as e:
                    logger.warning(f"å…³é—­ä¸Šä¸€é˜¶æ®µç¯å¢ƒæ—¶å‡ºé”™: {e}")
            
            # æ”¯æŒå¤šç¯å¢ƒå¹¶è¡Œï¼ˆä¸train()ä¸€è‡´ï¼‰
            training_strategy = self.training_config.get('training_strategy', {})
            num_parallel_envs = training_strategy.get('num_parallel_envs', 
                                                       self.training_config.get('num_parallel_envs', 1))
            
            # ä½¿ç”¨æ¨¡å—çº§å·¥å‚å‡½æ•°ï¼Œç¡®ä¿Windowsçš„spawnæ¨¡å¼å¯ä»¥pickle
            import platform
            log_dir_str = str(self.output_dir) if self.output_dir else None
            
            if num_parallel_envs > 1:
                env_fns = [
                    _make_env_factory(stage_sim_config, self.rl_config, log_dir_str, rank=i)
                    for i in range(num_parallel_envs)
                ]
                
                # Windowsä¸ŠSubprocVecEnvå¯èƒ½æœ‰é—®é¢˜ï¼Œæä¾›DummyVecEnvå›é€€
                if platform.system() == 'Windows':
                    try:
                        env = SubprocVecEnv(env_fns)
                        logger.info(f"  åˆ›å»ºäº† {num_parallel_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ (SubprocVecEnv)")
                    except Exception as e:
                        logger.warning(f"SubprocVecEnvåˆ›å»ºå¤±è´¥: {e}ï¼Œå›é€€åˆ°DummyVecEnv")
                        env = DummyVecEnv(env_fns)
                        logger.info(f"  åˆ›å»ºäº† {num_parallel_envs} ä¸ªç¯å¢ƒ (DummyVecEnvå›é€€)")
                else:
                    env = SubprocVecEnv(env_fns)
                    logger.info(f"  åˆ›å»ºäº† {num_parallel_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ (SubprocVecEnv)")
            else:
                # å•ç¯å¢ƒä½¿ç”¨å·¥å‚å‡½æ•°
                env = DummyVecEnv([_make_env_factory(stage_sim_config, self.rl_config, log_dir_str, rank=0)])
            
            # æ›´æ–°å½“å‰ç¯å¢ƒå¼•ç”¨ï¼ˆç”¨äºä¸‹ä¸€é˜¶æ®µå…³é—­ï¼‰
            current_env = env
            
            # åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆç”¨äºè¾¾æ ‡æ£€æµ‹ï¼Œä¸éœ€è¦Monitorï¼‰
            eval_env = DeliveryRLEnvironment(
                simulation_config=stage_sim_config,
                rl_config=self.rl_config
            )
            
            # åˆ›å»ºæˆ–æ›´æ–°æ¨¡å‹
            if model is None:
                model = self._create_ppo_model(env)
                # é…ç½®TensorBoard
                new_logger = configure(str(self.tensorboard_dir), ["stdout", "tensorboard"])
                model.set_logger(new_logger)
            else:
                # æ›´æ–°ç¯å¢ƒ
                model.set_env(env)
            
            # åˆ›å»ºå›è°ƒåˆ—è¡¨
            callbacks = []
            
            # 1. è®­ç»ƒç›‘æ§å›è°ƒï¼ˆä¸å«æ—©åœï¼‰
            monitor_callback = TrainingMonitorCallback(
                check_freq=1000,
                log_dir=str(self.output_dir),
                early_stop_patience=9999,  # å®é™…ä¸Šç¦ç”¨æ—©åœ
                min_improvement=0.01,
                verbose=1
            )
            callbacks.append(monitor_callback)
            
            # 2. è¯¾ç¨‹è¾¾æ ‡è·³è½¬å›è°ƒï¼ˆæ ¸å¿ƒï¼‰
            # ä»é…ç½®è¯»å–è¯„ä¼°å‚æ•°ï¼ˆå½±å“è¾¾æ ‡åˆ¤æ–­çš„ç¨³å®šæ€§ï¼‰
            eval_freq = curriculum_config.get('eval_freq', 5000)
            n_eval_episodes = curriculum_config.get('n_eval_episodes', 5)  # é»˜è®¤5ï¼Œå‡å°‘æ³¢åŠ¨
            
            curriculum_callback = CurriculumAdvanceCallback(
                eval_env=eval_env,
                min_completion_rate=stage.get('min_completion_rate', 0.5),
                max_timeout_rate=stage.get('max_timeout_rate', 0.5),
                eval_freq=eval_freq,
                n_eval_episodes=n_eval_episodes,
                min_timesteps=10000,  # è‡³å°‘è®­ç»ƒ1ä¸‡æ­¥æ‰æ£€æµ‹
                verbose=1
            )
            callbacks.append(curriculum_callback)
            
            # 3. æ£€æŸ¥ç‚¹å›è°ƒ
            checkpoint_callback = CheckpointCallback(
                save_freq=20000,
                save_path=str(self.output_dir / "checkpoints"),
                name_prefix=f"stage_{stage_idx + 1}"
            )
            callbacks.append(checkpoint_callback)
            
            callback_list = CallbackList(callbacks)
            
            # è®­ç»ƒè¯¥é˜¶æ®µï¼ˆå«å¼¹æ€§å»¶é•¿æœºåˆ¶ï¼‰
            stage_start_steps = model.num_timesteps if hasattr(model, 'num_timesteps') else 0
            max_retries = curriculum_config.get('max_retries', 2)  # ä»é…ç½®è¯»å–
            retry_count = 0
            extra_timesteps = curriculum_config.get('extra_timesteps', 50000)  # ä»é…ç½®è¯»å–
            failure_strategy = curriculum_config.get('failure_strategy', 'stop')  # å¤±è´¥ç­–ç•¥
            
            while retry_count <= max_retries:
                current_timesteps = stage['timesteps'] if retry_count == 0 else extra_timesteps
                
                if retry_count > 0:
                    logger.info(f"âš ï¸ é˜¶æ®µ {stage_idx + 1} æœªè¾¾æ ‡ï¼Œè¿›å…¥ç¬¬ {retry_count} æ¬¡åŠ æ—¶èµ› (+{extra_timesteps}æ­¥)...")
                
                try:
                    model.learn(
                        total_timesteps=current_timesteps,
                        callback=callback_list,
                        reset_num_timesteps=False,
                        progress_bar=True
                    )
                    
                    # 1. æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
                    if curriculum_callback.stage_completed:
                        logger.info(f"âœ… é˜¶æ®µ {stage_idx + 1} è¾¾æ ‡å®Œæˆ: {curriculum_callback.completion_reason}")
                        break # é€€å‡ºé‡è¯•å¾ªç¯ï¼Œè¿›å…¥ä¸‹ä¸€é˜¶æ®µ
                    
                    # 2. å¦‚æœæœªè¾¾æ ‡ï¼Œæ£€æŸ¥æ˜¯å¦å€¼å¾—åŠ æ—¶
                    best_rate = curriculum_callback.best_completion_rate
                    target_rate = stage.get('min_completion_rate', 0.5)
                    overtime_threshold = curriculum_config.get('overtime_threshold', 0.8)
                    threshold = target_rate * overtime_threshold  # ä»é…ç½®è¯»å–å®¹å¿åº¦
                    
                    if retry_count < max_retries:
                        if best_rate >= threshold:
                            logger.info(f"ğŸ“ˆ å½“å‰æœ€ä½³å®Œæˆç‡ {best_rate:.1%} æ¥è¿‘ç›®æ ‡ {target_rate:.1%}ï¼Œè§¦å‘è‡ªåŠ¨åŠ æ—¶")
                            retry_count += 1
                            continue
                        else:
                            logger.error(f"âŒ é˜¶æ®µ {stage_idx + 1} è®­ç»ƒå¤±è´¥ï¼æœ€ä½³å®Œæˆç‡ {best_rate:.1%} è¿œä½äºç›®æ ‡ {target_rate:.1%}")
                            logger.error("å»ºè®®ï¼šè°ƒæ•´è¯¾ç¨‹éš¾åº¦æˆ–æ£€æŸ¥æ¨¡å‹å‚æ•°")
                            if failure_strategy == 'stop':
                                return model  # ç»ˆæ­¢è®­ç»ƒ
                            else:
                                logger.warning(f"âš ï¸ failure_strategy='continue'ï¼Œè·³è¿‡é˜¶æ®µ {stage_idx + 1}ï¼Œç»§ç»­åç»­é˜¶æ®µ")
                                break  # è·³å‡ºåŠ æ—¶å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€é˜¶æ®µ
                    else:
                        logger.error(f"âŒ é˜¶æ®µ {stage_idx + 1} åŠ æ—¶èµ›è€—å°½ä»æœªè¾¾æ ‡")
                        if failure_strategy == 'stop':
                            return model
                        else:
                            logger.warning(f"âš ï¸ failure_strategy='continue'ï¼Œè·³è¿‡é˜¶æ®µ {stage_idx + 1}ï¼Œç»§ç»­åç»­é˜¶æ®µ")
                            break
                        
                except KeyboardInterrupt:
                    logger.info(f"\né˜¶æ®µ {stage_idx + 1} è¢«ç”¨æˆ·ä¸­æ–­")
                    interrupt_path = self.output_dir / f"interrupted_stage_{stage_idx + 1}"
                    model.save(interrupt_path)
                    logger.info(f"ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupt_path}")
                    return model
            
            # ä¿å­˜é˜¶æ®µæ¨¡å‹ï¼ˆåªæœ‰è¾¾æ ‡æˆ–è·³å‡ºå¾ªç¯åæ‰ä¿å­˜ï¼‰
            stage_model_path = self.output_dir / f"stage_{stage_idx + 1}_{stage['name']}"
            model.save(stage_model_path)
            logger.info(f"é˜¶æ®µæ¨¡å‹å·²ä¿å­˜: {stage_model_path}")
            
            total_stages_completed += 1
            
            # æ— è®ºå¦‚ä½•ï¼Œå…³é—­è¯„ä¼°ç¯å¢ƒ
            eval_env.close()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = self.output_dir / "final_curriculum_model"
        model.save(final_model_path)
        logger.info(f"\n{'='*60}")
        logger.info(f"è¯¾ç¨‹å­¦ä¹ å®Œæˆï¼")
        logger.info(f"  å®Œæˆé˜¶æ®µæ•°: {total_stages_completed}/{len(stages)}")
        logger.info(f"  æœ€ç»ˆæ¨¡å‹: {final_model_path}")
        logger.info(f"{'='*60}")
        
        # å…³é—­æœ€åä¸€ä¸ªè®­ç»ƒç¯å¢ƒ
        if current_env is not None:
            try:
                current_env.close()
                logger.debug("å·²å…³é—­æœ€ç»ˆè®­ç»ƒç¯å¢ƒ")
            except Exception as e:
                logger.warning(f"å…³é—­æœ€ç»ˆç¯å¢ƒæ—¶å‡ºé”™: {e}")
        
        return model
    
    def evaluate_model(self, model_path: str = None, n_episodes: int = 10):
        """
        è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹
        
        æ¨¡å‹è·¯å¾„æ£€æµ‹é€»è¾‘ï¼š
        1. å¦‚æœæŒ‡å®šäº†model_pathï¼Œç›´æ¥ä½¿ç”¨
        2. å¦åˆ™è‡ªåŠ¨æ£€æµ‹ï¼šfinal_curriculum_model.zip > final_model.zip > é»˜è®¤final_model
        3. è¿™æ ·æ— è®ºæ˜¯è¯¾ç¨‹å­¦ä¹ è¿˜æ˜¯å•åœºæ™¯è®­ç»ƒï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°æ¨¡å‹
        
        ç¯å¢ƒè¯´æ˜ï¼š
        - è¯„ä¼°ä½¿ç”¨éå‘é‡ç¯å¢ƒï¼ˆå•ç¯å¢ƒ + MonitoråŒ…è£…ï¼‰
        - ä¸è®­ç»ƒæ—¶çš„VecEnvç•¥æœ‰å·®å¼‚ï¼Œä½†SB3çš„predictå¯¹1D/2D obséƒ½èƒ½å¤„ç†
        - obs shapeä¿æŒä¸º(state_dim,)ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
        - å¦‚éœ€å®Œå…¨å¯¹ç§°ï¼Œå¯æ”¹ç”¨DummyVecEnvåŒ…è£…ï¼Œä½†ä¼šå¢åŠ ä»£ç å¤æ‚åº¦
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆNoneåˆ™è‡ªåŠ¨æ£€æµ‹æœ€æ–°è®­ç»ƒçš„æ¨¡å‹ï¼‰
            n_episodes: è¯„ä¼°Episodeæ•°
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«completion_rate, timeout_rate, mean_rewardç­‰
        """
        if not SB3_AVAILABLE:
            raise RuntimeError("Stable-Baselines3æœªå®‰è£…")
        
        # åŠ è½½æ¨¡å‹
        # æ³¨æ„ï¼štrain()ä¿å­˜åˆ°final_modelï¼Œtrain_with_curriculum()ä¿å­˜åˆ°final_curriculum_model
        # è‡ªåŠ¨æ£€æµ‹å“ªä¸ªå­˜åœ¨
        if model_path is None:
            curriculum_model = self.output_dir / "final_curriculum_model.zip"
            single_model = self.output_dir / "final_model.zip"
            
            if curriculum_model.exists():
                model_path = self.output_dir / "final_curriculum_model"
                logger.info("æ£€æµ‹åˆ°è¯¾ç¨‹å­¦ä¹ æ¨¡å‹")
            elif single_model.exists():
                model_path = self.output_dir / "final_model"
                logger.info("æ£€æµ‹åˆ°å•åœºæ™¯æ¨¡å‹")
            else:
                # é»˜è®¤å°è¯•final_modelï¼ˆå…¼å®¹æ—§è¡Œä¸ºï¼‰
                model_path = self.output_dir / "final_model"
                logger.warning("æœªæ‰¾åˆ°è®­ç»ƒæ¨¡å‹ï¼Œå°è¯•åŠ è½½é»˜è®¤è·¯å¾„")
        
        logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½PPOæ¨¡å‹
        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰ä¼ envç»™load()ï¼Œå› ä¸ºè¯„ä¼°åªéœ€è¦policyè¿›è¡Œpredict
        # å¦‚æœåç»­éœ€è¦ç»§ç»­è®­ç»ƒï¼ˆ.learn()ï¼‰æˆ–è·å–ç¯å¢ƒï¼ˆ.get_env()ï¼‰ï¼Œéœ€è¦å…ˆè°ƒç”¨model.set_env()
        model = PPO.load(model_path)
        
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆç‹¬ç«‹äºæ¨¡å‹ï¼Œä»…ç”¨äºé‡‡é›†è½¨è¿¹ï¼‰
        env = self.create_env()
        
        # è¿è¡Œè¯„ä¼°
        results = {
            'episode_rewards': [],
            'completion_rates': [],
            'timeout_rates': [],
            'episode_lengths': []
        }
        
        logger.info(f"è¿è¡Œ {n_episodes} ä¸ªè¯„ä¼°Episode...")
        
        for ep in range(n_episodes):
            obs, info = env.reset()
            done = False
            episode_reward = 0
            steps = 0
            
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward
                steps += 1
                done = terminated or truncated
            
            # æ”¶é›†ç»Ÿè®¡
            stats = env.get_episode_statistics()
            results['episode_rewards'].append(episode_reward)
            results['completion_rates'].append(stats.get('completion_rate', 0))
            results['timeout_rates'].append(stats.get('timeout_rate', 0))
            results['episode_lengths'].append(steps)
            
            logger.info(
                f"  Episode {ep + 1}: "
                f"reward={episode_reward:.2f}, "
                f"å®Œæˆç‡={stats.get('completion_rate', 0):.1%}, "
                f"è¶…æ—¶ç‡={stats.get('timeout_rate', 0):.1%}"
            )
        
        # æ±‡æ€»ç»“æœ
        summary = {
            'mean_reward': np.mean(results['episode_rewards']),
            'std_reward': np.std(results['episode_rewards']),
            'mean_completion_rate': np.mean(results['completion_rates']),
            'mean_timeout_rate': np.mean(results['timeout_rates']),
            'mean_episode_length': np.mean(results['episode_lengths'])
        }
        
        logger.info("\nè¯„ä¼°ç»“æœæ±‡æ€»:")
        logger.info(f"  å¹³å‡å¥–åŠ±: {summary['mean_reward']:.2f} Â± {summary['std_reward']:.2f}")
        logger.info(f"  å¹³å‡å®Œæˆç‡: {summary['mean_completion_rate']:.1%}")
        logger.info(f"  å¹³å‡è¶…æ—¶ç‡: {summary['mean_timeout_rate']:.1%}")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_path = self.output_dir / 'evaluation_results.json'
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump({**results, **summary}, f, indent=2)
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_path}")
        
        env.close()
        return summary


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Day 13: PPOè®­ç»ƒæµæ°´çº¿ - å³æ—¶é…é€å¼ºåŒ–å­¦ä¹ è°ƒåº¦'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config/rl_config.yaml',
        help='RLé…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--scenario',
        type=str,
        default='low_load',
        choices=['low_load', 'medium_load', 'high_load', 'extreme_load', 'low_stress'],
        help='è®­ç»ƒåœºæ™¯ (default: low_load)'
    )
    parser.add_argument(
        '--timesteps',
        type=int,
        default=None,
        help='è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®­ç»ƒæ­¥æ•°'
    )
    parser.add_argument(
        '--test-env',
        action='store_true',
        help='ä»…æµ‹è¯•ç¯å¢ƒå…¼å®¹æ€§'
    )
    parser.add_argument(
        '--curriculum',
        action='store_true',
        help='ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ ï¼ˆä»ä½è´Ÿè½½åˆ°é«˜è´Ÿè½½ï¼‰'
    )
    parser.add_argument(
        '--evaluate',
        type=str,
        default=None,
        help='è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹ï¼ˆæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼‰'
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='è°ƒè¯•æ¨¡å¼ï¼ˆå‡å°‘è®­ç»ƒæ­¥æ•°ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è°ƒè¯•æ¨¡å¼é…ç½®
    if args.debug:
        logger.info("è°ƒè¯•æ¨¡å¼ï¼šå‡å°‘è®­ç»ƒæ­¥æ•°")
        args.timesteps = args.timesteps or 5000
    
    if args.test_env:
        # ä»…æµ‹è¯•ç¯å¢ƒ
        print("="*60)
        print("æµ‹è¯•RLç¯å¢ƒå…¼å®¹æ€§")
        print("="*60)
        
        config_path = Path(args.config)
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ä½¿ç”¨ä½è´Ÿè½½åœºæ™¯æµ‹è¯•
        sim_config = config.get('simulation', {})
        sim_config.update(config.get('scenarios', {}).get('low_load', {}))
        
        env = DeliveryRLEnvironment(
            simulation_config=sim_config,
            rl_config=config.get('rl', {})
        )
        
        print(f"\nè§‚æµ‹ç©ºé—´: {env.observation_space}")
        print(f"  å½¢çŠ¶: {env.observation_space.shape}")
        print(f"åŠ¨ä½œç©ºé—´: {env.action_space}")
        # æ ¹æ®åŠ¨ä½œç©ºé—´ç±»å‹æ˜¾ç¤ºä¸åŒä¿¡æ¯
        if isinstance(env.action_space, spaces.Discrete):
            print(f"  å¤§å°: {env.action_space.n}")
        elif isinstance(env.action_space, spaces.MultiDiscrete):
            print(f"  ç»´åº¦: {env.action_space.nvec}  (æ¯ç»´åŠ¨ä½œæ•°)")
        elif isinstance(env.action_space, spaces.Box):
            print(f"  å½¢çŠ¶: {env.action_space.shape}")
        else:
            print(f"  ç±»å‹: {type(env.action_space)}")
        
        # æ£€æŸ¥ç¯å¢ƒ
        if SB3_AVAILABLE:
            print("\nè¿è¡ŒStable-Baselines3ç¯å¢ƒæ£€æŸ¥...")
            try:
                check_env(env)
                print("âœ“ ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
            except Exception as e:
                print(f"âš  ç¯å¢ƒæ£€æŸ¥è­¦å‘Š: {e}")
        
        # è¿è¡Œå‡ ä¸ªæ­¥éª¤
        print("\nè¿è¡Œæµ‹è¯•æ­¥éª¤...")
        obs, info = env.reset()
        print(f"åˆå§‹è§‚æµ‹å½¢çŠ¶: {obs.shape}")
        print(f"åˆå§‹ä¿¡æ¯: {info}")
        
        for i in range(3):
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            print(f"\næ­¥éª¤ {i+1}:")
            print(f"  åŠ¨ä½œ: {action}")
            print(f"  å¥–åŠ±: {reward:.4f}")
            print(f"  ç»ˆæ­¢: {terminated}, æˆªæ–­: {truncated}")
        
        env.close()
        print("\nâœ“ ç¯å¢ƒæµ‹è¯•å®Œæˆï¼")
        return
    
    if args.evaluate:
        # è¯„ä¼°æ¨¡å¼
        print("="*60)
        print("è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹")
        print("="*60)
        
        trainer = RLTrainer(args.config, scenario=args.scenario)
        trainer.evaluate_model(model_path=args.evaluate)
        return
    
    # è®­ç»ƒæ¨¡å¼
    print("="*60)
    print("Day 13: PPOè®­ç»ƒæµæ°´çº¿")
    print("="*60)
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"åœºæ™¯: {args.scenario}")
    print(f"è¯¾ç¨‹å­¦ä¹ : {args.curriculum}")
    print(f"è°ƒè¯•æ¨¡å¼: {args.debug}")
    
    trainer = RLTrainer(args.config, scenario=args.scenario)
    
    # è¦†ç›–è®­ç»ƒæ­¥æ•°
    if args.timesteps:
        trainer.training_config['total_timesteps'] = args.timesteps
        logger.info(f"è®­ç»ƒæ­¥æ•°å·²è¦†ç›–ä¸º: {args.timesteps:,}")
    
    # è¯¾ç¨‹å­¦ä¹ ï¼šCLIå‚æ•°ä¼˜å…ˆï¼Œå¦åˆ™ä»é…ç½®æ–‡ä»¶è¯»å–
    training_strategy = trainer.training_config.get('training_strategy', {})
    use_curriculum = args.curriculum or training_strategy.get('use_curriculum_learning', False)
    
    if use_curriculum:
        # è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
        trainer.train_with_curriculum()
    else:
        # æ ‡å‡†è®­ç»ƒ
        trainer.train()
    
    print("\nè®­ç»ƒå®Œæˆï¼")
    print(f"è¾“å‡ºç›®å½•: {trainer.output_dir}")
    print(f"TensorBoardå‘½ä»¤: tensorboard --logdir={trainer.tensorboard_dir}")


if __name__ == "__main__":
    main()
